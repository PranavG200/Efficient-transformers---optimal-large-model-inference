# Efficient Transformers : Post Training Pruning and Quantization for optimal large model inference
Enhancing transformer inference by employing post-training pruning (PTP) and quantization (PTQ). We explore how PT inference optimization methods can reduce computational complexity, memory footprint &amp; inference latency and make things run faster while using less memory
